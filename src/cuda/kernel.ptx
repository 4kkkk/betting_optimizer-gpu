//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35404655
// Cuda compilation tools, release 12.8, V12.8.61
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_86
.address_size 64

	// .globl	optimize_kernel

.visible .entry optimize_kernel(
	.param .u64 optimize_kernel_param_0,
	.param .u64 optimize_kernel_param_1,
	.param .u64 optimize_kernel_param_2,
	.param .u32 optimize_kernel_param_3,
	.param .u32 optimize_kernel_param_4,
	.param .u32 optimize_kernel_param_5,
	.param .f64 optimize_kernel_param_6
)
{
	.reg .pred 	%p<40>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<110>;
	.reg .f64 	%fd<115>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd6, [optimize_kernel_param_0];
	ld.param.u64 	%rd4, [optimize_kernel_param_1];
	ld.param.u64 	%rd5, [optimize_kernel_param_2];
	ld.param.u32 	%r33, [optimize_kernel_param_3];
	ld.param.u32 	%r35, [optimize_kernel_param_4];
	ld.param.u32 	%r34, [optimize_kernel_param_5];
	ld.param.f64 	%fd34, [optimize_kernel_param_6];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r36, %ntid.x;
	mov.u32 	%r37, %ctaid.x;
	mov.u32 	%r38, %tid.x;
	mad.lo.s32 	%r1, %r37, %r36, %r38;
	setp.ge.s32 	%p4, %r1, %r35;
	@%p4 bra 	$L__BB0_32;

	mul.lo.s32 	%r42, %r1, 7;
	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r42, 8;
	add.s64 	%rd2, %rd7, %rd8;
	ld.global.nc.f64 	%fd35, [%rd2];
	cvt.rmi.s32.f64 	%r2, %fd35;
	ld.global.nc.f64 	%fd1, [%rd2+16];
	ld.global.nc.f64 	%fd2, [%rd2+24];
	ld.global.nc.f64 	%fd3, [%rd2+32];
	ld.global.nc.f64 	%fd36, [%rd2+48];
	cvt.rmi.s32.f64 	%r3, %fd36;
	cvta.to.global.u64 	%rd9, %rd5;
	mul.wide.s32 	%rd10, %r1, 48;
	add.s64 	%rd11, %rd9, %rd10;
	add.s64 	%rd3, %rd11, 40;
	ld.global.f64 	%fd4, [%rd11+40];
	mul.f64 	%fd37, %fd4, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r43}, %fd37;
	}
	and.b32  	%r44, %r43, -2147483648;
	mov.f64 	%fd38, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r4}, %fd38;
	}
	or.b32  	%r45, %r4, %r44;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r5, %temp}, %fd38;
	}
	mov.b64 	%fd39, {%r5, %r45};
	add.rz.f64 	%fd40, %fd37, %fd39;
	cvt.rzi.f64.f64 	%fd41, %fd40;
	div.rn.f64 	%fd111, %fd41, 0d4059000000000000;
	setp.ge.s32 	%p6, %r2, %r33;
	mov.pred 	%p5, -1;
	mov.u32 	%r103, 0;
	mov.f64 	%fd112, %fd111;
	mov.u32 	%r105, %r103;
	mov.u32 	%r104, %r103;
	mov.pred 	%p39, %p5;
	@%p6 bra 	$L__BB0_29;

	ld.global.nc.f64 	%fd6, [%rd2+8];
	mul.f64 	%fd7, %fd34, 0d4059000000000000;
	ld.global.nc.f64 	%fd42, [%rd2+40];
	div.rn.f64 	%fd8, %fd42, 0d4059000000000000;
	add.s32 	%r6, %r33, -1;
	mov.u32 	%r105, 0;
	mov.u32 	%r106, %r2;
	mov.u32 	%r104, %r105;
	mov.u32 	%r103, %r105;
	mov.f64 	%fd112, %fd111;

$L__BB0_3:
	setp.lt.s32 	%p7, %r2, 1;
	mov.u16 	%rs4, 1;
	@%p7 bra 	$L__BB0_8;

	mov.u32 	%r97, 0;

$L__BB0_5:
	setp.le.s32 	%p9, %r106, %r97;
	mov.pred 	%p38, -1;
	@%p9 bra 	$L__BB0_7;

	not.b32 	%r50, %r97;
	add.s32 	%r51, %r106, %r50;
	mul.wide.s32 	%rd12, %r51, 8;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.f64 	%fd43, [%rd13];
	setp.gt.f64 	%p38, %fd43, %fd6;

$L__BB0_7:
	not.pred 	%p10, %p38;
	selp.u16 	%rs4, 1, 0, %p10;
	add.s32 	%r97, %r97, 1;
	setp.lt.s32 	%p11, %r97, %r2;
	and.pred  	%p12, %p11, %p10;
	@%p12 bra 	$L__BB0_5;

$L__BB0_8:
	setp.ge.s32 	%p13, %r106, %r33;
	setp.eq.s16 	%p14, %rs4, 0;
	or.pred  	%p15, %p14, %p13;
	@%p15 bra 	$L__BB0_28;

	mov.u32 	%r98, %r106;

$L__BB0_10:
	mul.wide.s32 	%rd14, %r98, 8;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.nc.f64 	%fd11, [%rd15];
	setp.lt.f64 	%p16, %fd11, %fd1;
	@%p16 bra 	$L__BB0_27;
	bra.uni 	$L__BB0_11;

$L__BB0_27:
	add.s32 	%r98, %r98, 1;
	setp.lt.s32 	%p35, %r98, %r33;
	@%p35 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_28;

$L__BB0_11:
	setp.ltu.f64 	%p17, %fd11, %fd1;
	@%p17 bra 	$L__BB0_28;

	setp.eq.s32 	%p18, %r34, 0;
	add.s32 	%r104, %r104, 1;
	@%p18 bra 	$L__BB0_14;

	mul.f64 	%fd44, %fd8, %fd111;
	mul.f64 	%fd45, %fd44, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r52}, %fd45;
	}
	and.b32  	%r53, %r52, -2147483648;
	or.b32  	%r54, %r4, %r53;
	mov.b64 	%fd46, {%r5, %r54};
	add.rz.f64 	%fd47, %fd45, %fd46;
	cvt.rzi.f64.f64 	%fd106, %fd47;
	bra.uni 	$L__BB0_15;

$L__BB0_14:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd7;
	}
	and.b32  	%r56, %r55, -2147483648;
	or.b32  	%r57, %r4, %r56;
	mov.b64 	%fd48, {%r5, %r57};
	add.rz.f64 	%fd49, %fd7, %fd48;
	cvt.rzi.f64.f64 	%fd106, %fd49;

$L__BB0_15:
	setp.lt.s32 	%p19, %r3, 1;
	@%p19 bra 	$L__BB0_19;

	mov.u32 	%r99, 0;
	mov.f64 	%fd107, %fd111;

$L__BB0_17:
	div.rn.f64 	%fd17, %fd106, 0d4059000000000000;
	setp.gt.f64 	%p21, %fd17, %fd107;
	mov.pred 	%p39, 0;
	@%p21 bra 	$L__BB0_29;

	sub.f64 	%fd50, %fd107, %fd17;
	mul.f64 	%fd51, %fd50, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r59}, %fd51;
	}
	and.b32  	%r60, %r59, -2147483648;
	or.b32  	%r61, %r4, %r60;
	mov.b64 	%fd52, {%r5, %r61};
	add.rz.f64 	%fd53, %fd51, %fd52;
	cvt.rzi.f64.f64 	%fd54, %fd53;
	div.rn.f64 	%fd107, %fd54, 0d4059000000000000;
	mul.f64 	%fd55, %fd3, %fd17;
	mul.f64 	%fd56, %fd55, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r62}, %fd56;
	}
	and.b32  	%r63, %r62, -2147483648;
	or.b32  	%r64, %r4, %r63;
	mov.b64 	%fd57, {%r5, %r64};
	add.rz.f64 	%fd58, %fd56, %fd57;
	cvt.rzi.f64.f64 	%fd106, %fd58;
	add.s32 	%r99, %r99, 1;
	setp.lt.s32 	%p22, %r99, %r3;
	@%p22 bra 	$L__BB0_17;

$L__BB0_19:
	@%p18 bra 	$L__BB0_21;

	mul.f64 	%fd59, %fd8, %fd111;
	mul.f64 	%fd60, %fd59, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r65}, %fd60;
	}
	and.b32  	%r66, %r65, -2147483648;
	or.b32  	%r67, %r4, %r66;
	mov.b64 	%fd61, {%r5, %r67};
	add.rz.f64 	%fd62, %fd60, %fd61;
	cvt.rzi.f64.f64 	%fd108, %fd62;
	bra.uni 	$L__BB0_22;

$L__BB0_21:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r68}, %fd7;
	}
	and.b32  	%r69, %r68, -2147483648;
	or.b32  	%r70, %r4, %r69;
	mov.b64 	%fd63, {%r5, %r70};
	add.rz.f64 	%fd64, %fd7, %fd63;
	cvt.rzi.f64.f64 	%fd108, %fd64;

$L__BB0_22:
	setp.ge.s32 	%p24, %r98, %r6;
	or.pred  	%p26, %p19, %p24;
	div.rn.f64 	%fd109, %fd108, 0d4059000000000000;
	setp.gt.f64 	%p27, %fd109, %fd111;
	or.pred  	%p28, %p27, %p26;
	mov.u32 	%r106, %r98;
	@%p28 bra 	$L__BB0_28;

	mov.u32 	%r101, 0;
	mov.u32 	%r106, %r98;

$L__BB0_24:
	add.s32 	%r103, %r103, 1;
	sub.f64 	%fd65, %fd111, %fd109;
	mul.f64 	%fd66, %fd65, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r72}, %fd66;
	}
	and.b32  	%r73, %r72, -2147483648;
	or.b32  	%r74, %r4, %r73;
	mov.b64 	%fd67, {%r5, %r74};
	add.rz.f64 	%fd68, %fd66, %fd67;
	cvt.rzi.f64.f64 	%fd69, %fd68;
	div.rn.f64 	%fd111, %fd69, 0d4059000000000000;
	add.s32 	%r106, %r106, 1;
	mul.wide.s32 	%rd16, %r106, 8;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.f64 	%fd70, [%rd17];
	setp.ltu.f64 	%p29, %fd70, %fd2;
	@%p29 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_25;

$L__BB0_26:
	mul.f64 	%fd82, %fd3, %fd109;
	mul.f64 	%fd83, %fd82, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r81}, %fd83;
	}
	and.b32  	%r82, %r81, -2147483648;
	or.b32  	%r83, %r4, %r82;
	mov.b64 	%fd84, {%r5, %r83};
	add.rz.f64 	%fd85, %fd83, %fd84;
	cvt.rzi.f64.f64 	%fd86, %fd85;
	div.rn.f64 	%fd109, %fd86, 0d4059000000000000;
	add.s32 	%r101, %r101, 1;
	setp.ge.s32 	%p30, %r101, %r3;
	setp.ge.s32 	%p31, %r106, %r6;
	or.pred  	%p32, %p30, %p31;
	setp.gt.f64 	%p33, %fd109, %fd111;
	or.pred  	%p34, %p33, %p32;
	@%p34 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_24;

$L__BB0_25:
	mul.f64 	%fd71, %fd2, %fd109;
	mul.f64 	%fd72, %fd71, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r75}, %fd72;
	}
	and.b32  	%r76, %r75, -2147483648;
	or.b32  	%r77, %r4, %r76;
	mov.b64 	%fd73, {%r5, %r77};
	add.rz.f64 	%fd74, %fd72, %fd73;
	cvt.rzi.f64.f64 	%fd75, %fd74;
	div.rn.f64 	%fd76, %fd75, 0d4059000000000000;
	add.f64 	%fd77, %fd111, %fd76;
	mul.f64 	%fd78, %fd77, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r78}, %fd78;
	}
	and.b32  	%r79, %r78, -2147483648;
	or.b32  	%r80, %r4, %r79;
	mov.b64 	%fd79, {%r5, %r80};
	add.rz.f64 	%fd80, %fd78, %fd79;
	cvt.rzi.f64.f64 	%fd81, %fd80;
	div.rn.f64 	%fd111, %fd81, 0d4059000000000000;
	add.s32 	%r105, %r105, 1;
	max.f64 	%fd112, %fd111, %fd112;

$L__BB0_28:
	add.s32 	%r106, %r106, 1;
	setp.lt.s32 	%p37, %r106, %r33;
	mov.pred 	%p39, %p5;
	@%p37 bra 	$L__BB0_3;

$L__BB0_29:
	@%p39 bra 	$L__BB0_31;
	bra.uni 	$L__BB0_30;

$L__BB0_31:
	mul.f64 	%fd87, %fd111, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r84}, %fd87;
	}
	and.b32  	%r85, %r84, -2147483648;
	or.b32  	%r86, %r4, %r85;
	mov.b64 	%fd88, {%r5, %r86};
	add.rz.f64 	%fd89, %fd87, %fd88;
	cvt.rzi.f64.f64 	%fd90, %fd89;
	div.rn.f64 	%fd91, %fd90, 0d4059000000000000;
	st.global.f64 	[%rd3+-40], %fd91;
	mul.f64 	%fd92, %fd112, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r87}, %fd92;
	}
	and.b32  	%r88, %r87, -2147483648;
	or.b32  	%r89, %r4, %r88;
	mov.b64 	%fd93, {%r5, %r89};
	add.rz.f64 	%fd94, %fd92, %fd93;
	cvt.rzi.f64.f64 	%fd95, %fd94;
	div.rn.f64 	%fd96, %fd95, 0d4059000000000000;
	st.global.f64 	[%rd3+-32], %fd96;
	st.global.v2.u32 	[%rd3+-24], {%r103, %r104};
	st.global.u32 	[%rd3+-16], %r105;
	sub.f64 	%fd97, %fd111, %fd4;
	mul.f64 	%fd98, %fd97, 0d4059000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r90}, %fd98;
	}
	and.b32  	%r91, %r90, -2147483648;
	or.b32  	%r92, %r4, %r91;
	mov.b64 	%fd99, {%r5, %r92};
	add.rz.f64 	%fd100, %fd98, %fd99;
	cvt.rzi.f64.f64 	%fd101, %fd100;
	div.rn.f64 	%fd102, %fd101, 0d4059000000000000;
	st.global.f64 	[%rd3+-8], %fd102;
	bra.uni 	$L__BB0_32;

$L__BB0_30:
	mov.u64 	%rd18, -4616189618054758400;
	st.global.u64 	[%rd3+-8], %rd18;

$L__BB0_32:
	ret;

}

