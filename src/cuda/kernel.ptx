//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35404655
// Cuda compilation tools, release 12.8, V12.8.61
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_86
.address_size 64

	// .globl	optimize_kernel

.visible .entry optimize_kernel(
	.param .u64 optimize_kernel_param_0,
	.param .u64 optimize_kernel_param_1,
	.param .u64 optimize_kernel_param_2,
	.param .u32 optimize_kernel_param_3,
	.param .u32 optimize_kernel_param_4,
	.param .u32 optimize_kernel_param_5,
	.param .f64 optimize_kernel_param_6
)
{
	.reg .pred 	%p<74>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<112>;
	.reg .f64 	%fd<77>;
	.reg .b64 	%rd<27>;


	ld.param.u64 	%rd5, [optimize_kernel_param_0];
	ld.param.u64 	%rd3, [optimize_kernel_param_1];
	ld.param.u64 	%rd4, [optimize_kernel_param_2];
	ld.param.u32 	%r56, [optimize_kernel_param_3];
	ld.param.u32 	%r58, [optimize_kernel_param_4];
	ld.param.u32 	%r57, [optimize_kernel_param_5];
	ld.param.f64 	%fd47, [optimize_kernel_param_6];
	cvta.to.global.u64 	%rd1, %rd5;
	mov.u32 	%r59, %ntid.x;
	mov.u32 	%r60, %ctaid.x;
	mov.u32 	%r61, %tid.x;
	mad.lo.s32 	%r1, %r60, %r59, %r61;
	setp.ge.s32 	%p6, %r1, %r58;
	@%p6 bra 	$L__BB0_56;

	mul.lo.s32 	%r62, %r1, 7;
	cvta.to.global.u64 	%rd6, %rd3;
	mul.wide.s32 	%rd7, %r62, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.f64 	%fd48, [%rd8];
	cvt.rmi.s32.f64 	%r2, %fd48;
	ld.global.nc.f64 	%fd1, [%rd8+8];
	ld.global.nc.f64 	%fd2, [%rd8+16];
	ld.global.nc.f64 	%fd3, [%rd8+24];
	ld.global.nc.f64 	%fd4, [%rd8+32];
	ld.global.nc.f64 	%fd5, [%rd8+40];
	ld.global.nc.f64 	%fd49, [%rd8+48];
	cvt.rmi.s32.f64 	%r3, %fd49;
	cvta.to.global.u64 	%rd9, %rd4;
	mul.wide.s32 	%rd10, %r1, 48;
	add.s64 	%rd11, %rd9, %rd10;
	add.s64 	%rd2, %rd11, 40;
	ld.global.f64 	%fd6, [%rd11+40];
	setp.lt.s32 	%p7, %r2, %r56;
	@%p7 bra 	$L__BB0_3;
	bra.uni 	$L__BB0_2;

$L__BB0_3:
	setp.eq.s32 	%p8, %r57, 0;
	mov.f64 	%fd57, %fd47;
	@%p8 bra 	$L__BB0_5;

	div.rn.f64 	%fd50, %fd5, 0d4059000000000000;
	mul.f64 	%fd57, %fd50, %fd6;

$L__BB0_5:
	setp.lt.s32 	%p9, %r3, 1;
	@%p9 bra 	$L__BB0_9;

	mov.u32 	%r80, 0;
	mov.f64 	%fd58, %fd6;

$L__BB0_7:
	setp.gt.f64 	%p10, %fd57, %fd58;
	@%p10 bra 	$L__BB0_55;

	sub.f64 	%fd58, %fd58, %fd57;
	mul.f64 	%fd57, %fd4, %fd57;
	add.s32 	%r80, %r80, 1;
	setp.lt.s32 	%p11, %r80, %r3;
	@%p11 bra 	$L__BB0_7;

$L__BB0_9:
	add.s32 	%r6, %r56, -1;
	@%p8 bra 	$L__BB0_31;

	div.rn.f64 	%fd13, %fd5, 0d4059000000000000;
	mov.u32 	%r93, 0;
	mov.u32 	%r94, %r2;
	mov.u32 	%r92, %r93;
	mov.u32 	%r91, %r93;
	mov.f64 	%fd66, %fd6;
	mov.f64 	%fd65, %fd6;

$L__BB0_11:
	setp.lt.s32 	%p13, %r2, 1;
	mov.u16 	%rs7, 1;
	@%p13 bra 	$L__BB0_16;

	mov.u32 	%r85, 0;

$L__BB0_13:
	setp.le.s32 	%p15, %r94, %r85;
	mov.pred 	%p71, -1;
	@%p15 bra 	$L__BB0_15;

	not.b32 	%r68, %r85;
	add.s32 	%r69, %r94, %r68;
	mul.wide.s32 	%rd13, %r69, 8;
	add.s64 	%rd14, %rd1, %rd13;
	ld.global.nc.f64 	%fd51, [%rd14];
	setp.gt.f64 	%p71, %fd51, %fd1;

$L__BB0_15:
	not.pred 	%p16, %p71;
	selp.u16 	%rs7, 1, 0, %p16;
	add.s32 	%r85, %r85, 1;
	setp.lt.s32 	%p17, %r85, %r2;
	and.pred  	%p18, %p17, %p16;
	@%p18 bra 	$L__BB0_13;

$L__BB0_16:
	setp.ge.s32 	%p19, %r94, %r56;
	setp.eq.s16 	%p20, %rs7, 0;
	or.pred  	%p21, %p20, %p19;
	@%p21 bra 	$L__BB0_30;

	mov.u32 	%r86, %r94;

$L__BB0_18:
	mul.wide.s32 	%rd15, %r86, 8;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.nc.f64 	%fd16, [%rd16];
	setp.lt.f64 	%p22, %fd16, %fd2;
	@%p22 bra 	$L__BB0_29;
	bra.uni 	$L__BB0_19;

$L__BB0_29:
	add.s32 	%r86, %r86, 1;
	setp.lt.s32 	%p39, %r86, %r56;
	@%p39 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_30;

$L__BB0_19:
	setp.ltu.f64 	%p23, %fd16, %fd2;
	@%p23 bra 	$L__BB0_30;

	add.s32 	%r92, %r92, 1;
	mul.f64 	%fd63, %fd13, %fd66;
	@%p9 bra 	$L__BB0_24;

	mov.u32 	%r87, 0;
	mov.f64 	%fd61, %fd63;
	mov.f64 	%fd62, %fd66;

$L__BB0_22:
	setp.gt.f64 	%p26, %fd61, %fd62;
	mov.pred 	%p73, 0;
	@%p26 bra 	$L__BB0_52;

	sub.f64 	%fd62, %fd62, %fd61;
	mul.f64 	%fd61, %fd4, %fd61;
	add.s32 	%r87, %r87, 1;
	setp.lt.s32 	%p27, %r87, %r3;
	@%p27 bra 	$L__BB0_22;

$L__BB0_24:
	setp.ge.s32 	%p29, %r86, %r6;
	or.pred  	%p30, %p9, %p29;
	setp.gt.f64 	%p31, %fd63, %fd66;
	or.pred  	%p32, %p31, %p30;
	mov.u32 	%r94, %r86;
	@%p32 bra 	$L__BB0_30;

	mov.u32 	%r89, 0;
	mov.u32 	%r94, %r86;

$L__BB0_26:
	add.s32 	%r91, %r91, 1;
	sub.f64 	%fd66, %fd66, %fd63;
	add.s32 	%r94, %r94, 1;
	mul.wide.s32 	%rd17, %r94, 8;
	add.s64 	%rd18, %rd1, %rd17;
	ld.global.nc.f64 	%fd52, [%rd18];
	setp.ltu.f64 	%p33, %fd52, %fd3;
	@%p33 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_27;

$L__BB0_28:
	mul.f64 	%fd63, %fd4, %fd63;
	add.s32 	%r89, %r89, 1;
	setp.ge.s32 	%p34, %r89, %r3;
	setp.ge.s32 	%p35, %r94, %r6;
	or.pred  	%p36, %p34, %p35;
	setp.gt.f64 	%p37, %fd63, %fd66;
	or.pred  	%p38, %p37, %p36;
	@%p38 bra 	$L__BB0_30;
	bra.uni 	$L__BB0_26;

$L__BB0_27:
	fma.rn.f64 	%fd66, %fd3, %fd63, %fd66;
	add.s32 	%r93, %r93, 1;
	max.f64 	%fd65, %fd66, %fd65;

$L__BB0_30:
	add.s32 	%r94, %r94, 1;
	setp.lt.s32 	%p41, %r94, %r56;
	mov.pred 	%p73, -1;
	@%p41 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_52;

$L__BB0_2:
	mov.u64 	%rd12, -4616189618054758400;
	st.global.u64 	[%rd2+-8], %rd12;
	bra.uni 	$L__BB0_56;

$L__BB0_55:
	mov.u64 	%rd26, -4616189618054758400;
	st.global.u64 	[%rd2+-8], %rd26;
	bra.uni 	$L__BB0_56;

$L__BB0_31:
	mov.u32 	%r93, 0;
	mov.u32 	%r108, %r2;
	mov.u32 	%r92, %r93;
	mov.u32 	%r91, %r93;
	mov.f64 	%fd66, %fd6;
	mov.f64 	%fd65, %fd6;

$L__BB0_32:
	setp.lt.s32 	%p42, %r2, 1;
	mov.u16 	%rs8, 1;
	@%p42 bra 	$L__BB0_37;

	mov.u32 	%r99, 0;

$L__BB0_34:
	setp.le.s32 	%p44, %r108, %r99;
	mov.pred 	%p72, -1;
	@%p44 bra 	$L__BB0_36;

	not.b32 	%r76, %r99;
	add.s32 	%r77, %r108, %r76;
	mul.wide.s32 	%rd19, %r77, 8;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.nc.f64 	%fd53, [%rd20];
	setp.gt.f64 	%p72, %fd53, %fd1;

$L__BB0_36:
	not.pred 	%p45, %p72;
	selp.u16 	%rs8, 1, 0, %p45;
	add.s32 	%r99, %r99, 1;
	setp.lt.s32 	%p46, %r99, %r2;
	and.pred  	%p47, %p46, %p45;
	@%p47 bra 	$L__BB0_34;

$L__BB0_37:
	setp.ge.s32 	%p48, %r108, %r56;
	setp.eq.s16 	%p49, %rs8, 0;
	or.pred  	%p50, %p49, %p48;
	@%p50 bra 	$L__BB0_51;

	mov.u32 	%r100, %r108;

$L__BB0_39:
	mul.wide.s32 	%rd21, %r100, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.nc.f64 	%fd32, [%rd22];
	setp.lt.f64 	%p51, %fd32, %fd2;
	@%p51 bra 	$L__BB0_50;
	bra.uni 	$L__BB0_40;

$L__BB0_50:
	add.s32 	%r100, %r100, 1;
	setp.lt.s32 	%p68, %r100, %r56;
	@%p68 bra 	$L__BB0_39;
	bra.uni 	$L__BB0_51;

$L__BB0_40:
	setp.ltu.f64 	%p52, %fd32, %fd2;
	@%p52 bra 	$L__BB0_51;

	add.s32 	%r92, %r92, 1;
	@%p9 bra 	$L__BB0_45;

	mov.u32 	%r101, 0;
	mov.f64 	%fd69, %fd47;
	mov.f64 	%fd70, %fd66;

$L__BB0_43:
	setp.gt.f64 	%p55, %fd69, %fd70;
	mov.pred 	%p73, 0;
	@%p55 bra 	$L__BB0_52;

	sub.f64 	%fd70, %fd70, %fd69;
	mul.f64 	%fd69, %fd4, %fd69;
	add.s32 	%r101, %r101, 1;
	setp.lt.s32 	%p56, %r101, %r3;
	@%p56 bra 	$L__BB0_43;

$L__BB0_45:
	setp.ge.s32 	%p58, %r100, %r6;
	or.pred  	%p59, %p9, %p58;
	setp.lt.f64 	%p60, %fd66, %fd47;
	or.pred  	%p61, %p60, %p59;
	mov.u32 	%r108, %r100;
	@%p61 bra 	$L__BB0_51;

	mov.u32 	%r103, 0;
	mov.u32 	%r108, %r100;
	mov.f64 	%fd71, %fd47;

$L__BB0_47:
	add.s32 	%r91, %r91, 1;
	sub.f64 	%fd66, %fd66, %fd71;
	add.s32 	%r108, %r108, 1;
	mul.wide.s32 	%rd23, %r108, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.nc.f64 	%fd54, [%rd24];
	setp.ltu.f64 	%p62, %fd54, %fd3;
	@%p62 bra 	$L__BB0_49;
	bra.uni 	$L__BB0_48;

$L__BB0_49:
	mul.f64 	%fd71, %fd4, %fd71;
	add.s32 	%r103, %r103, 1;
	setp.ge.s32 	%p63, %r103, %r3;
	setp.ge.s32 	%p64, %r108, %r6;
	or.pred  	%p65, %p63, %p64;
	setp.gt.f64 	%p66, %fd71, %fd66;
	or.pred  	%p67, %p66, %p65;
	@%p67 bra 	$L__BB0_51;
	bra.uni 	$L__BB0_47;

$L__BB0_48:
	fma.rn.f64 	%fd66, %fd3, %fd71, %fd66;
	add.s32 	%r93, %r93, 1;
	max.f64 	%fd65, %fd66, %fd65;

$L__BB0_51:
	add.s32 	%r108, %r108, 1;
	setp.lt.s32 	%p70, %r108, %r56;
	mov.pred 	%p73, -1;
	@%p70 bra 	$L__BB0_32;

$L__BB0_52:
	@%p73 bra 	$L__BB0_54;
	bra.uni 	$L__BB0_53;

$L__BB0_54:
	st.global.f64 	[%rd2+-40], %fd66;
	st.global.f64 	[%rd2+-32], %fd65;
	st.global.v2.u32 	[%rd2+-24], {%r91, %r92};
	st.global.u32 	[%rd2+-16], %r93;
	sub.f64 	%fd55, %fd66, %fd6;
	st.global.f64 	[%rd2+-8], %fd55;
	bra.uni 	$L__BB0_56;

$L__BB0_53:
	mov.u64 	%rd25, -4616189618054758400;
	st.global.u64 	[%rd2+-8], %rd25;

$L__BB0_56:
	ret;

}

