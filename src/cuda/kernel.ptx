//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-35404655
// Cuda compilation tools, release 12.8, V12.8.61
// Based on NVVM 7.0.1
//

.version 8.7
.target sm_86
.address_size 64

	// .globl	optimize_kernel

.visible .entry optimize_kernel(
	.param .u64 optimize_kernel_param_0,
	.param .u64 optimize_kernel_param_1,
	.param .u64 optimize_kernel_param_2,
	.param .u32 optimize_kernel_param_3,
	.param .u32 optimize_kernel_param_4,
	.param .u32 optimize_kernel_param_5,
	.param .f64 optimize_kernel_param_6
)
{
	.reg .pred 	%p<50>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<103>;
	.reg .f64 	%fd<55>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd6, [optimize_kernel_param_0];
	ld.param.u64 	%rd4, [optimize_kernel_param_1];
	ld.param.u64 	%rd5, [optimize_kernel_param_2];
	ld.param.u32 	%r50, [optimize_kernel_param_3];
	ld.param.u32 	%r52, [optimize_kernel_param_4];
	ld.param.u32 	%r51, [optimize_kernel_param_5];
	ld.param.f64 	%fd32, [optimize_kernel_param_6];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r53, %ntid.x;
	mov.u32 	%r54, %ctaid.x;
	mov.u32 	%r55, %tid.x;
	mad.lo.s32 	%r1, %r54, %r53, %r55;
	setp.ge.s32 	%p5, %r1, %r52;
	@%p5 bra 	$L__BB0_38;

	mul.lo.s32 	%r59, %r1, 7;
	cvta.to.global.u64 	%rd7, %rd4;
	mul.wide.s32 	%rd8, %r59, 8;
	add.s64 	%rd2, %rd7, %rd8;
	ld.global.nc.f64 	%fd33, [%rd2];
	cvt.rmi.s32.f64 	%r2, %fd33;
	ld.global.nc.f64 	%fd1, [%rd2+8];
	ld.global.nc.f64 	%fd2, [%rd2+16];
	ld.global.nc.f64 	%fd3, [%rd2+24];
	ld.global.nc.f64 	%fd4, [%rd2+32];
	ld.global.nc.f64 	%fd34, [%rd2+48];
	cvt.rmi.s32.f64 	%r3, %fd34;
	cvta.to.global.u64 	%rd9, %rd5;
	mul.wide.s32 	%rd10, %r1, 48;
	add.s64 	%rd11, %rd9, %rd10;
	add.s64 	%rd3, %rd11, 40;
	ld.global.f64 	%fd5, [%rd11+40];
	setp.ge.s32 	%p6, %r2, %r50;
	mov.u32 	%r83, 0;
	mov.f64 	%fd45, %fd5;
	mov.f64 	%fd46, %fd5;
	mov.u32 	%r84, %r83;
	mov.u32 	%r85, %r83;
	@%p6 bra 	$L__BB0_37;

	add.s32 	%r4, %r50, -1;
	setp.eq.s32 	%p7, %r51, 0;
	@%p7 bra 	$L__BB0_20;

	ld.global.nc.f64 	%fd35, [%rd2+40];
	div.rn.f64 	%fd6, %fd35, 0d4059000000000000;
	mov.u32 	%r85, 0;
	mov.u32 	%r86, %r2;
	mov.u32 	%r84, %r85;
	mov.u32 	%r83, %r85;
	mov.f64 	%fd46, %fd5;
	mov.f64 	%fd45, %fd5;

$L__BB0_4:
	setp.lt.s32 	%p8, %r2, 1;
	mov.u16 	%rs7, 1;
	@%p8 bra 	$L__BB0_9;

	mov.u32 	%r78, 0;

$L__BB0_6:
	setp.le.s32 	%p10, %r86, %r78;
	mov.pred 	%p48, -1;
	@%p10 bra 	$L__BB0_8;

	not.b32 	%r64, %r78;
	add.s32 	%r65, %r86, %r64;
	mul.wide.s32 	%rd12, %r65, 8;
	add.s64 	%rd13, %rd1, %rd12;
	ld.global.nc.f64 	%fd36, [%rd13];
	setp.gt.f64 	%p48, %fd36, %fd1;

$L__BB0_8:
	not.pred 	%p11, %p48;
	selp.u16 	%rs7, 1, 0, %p11;
	add.s32 	%r78, %r78, 1;
	setp.lt.s32 	%p12, %r78, %r2;
	and.pred  	%p13, %p12, %p11;
	@%p13 bra 	$L__BB0_6;

$L__BB0_9:
	setp.ge.s32 	%p14, %r86, %r50;
	setp.eq.s16 	%p15, %rs7, 0;
	or.pred  	%p16, %p15, %p14;
	@%p16 bra 	$L__BB0_19;

	mov.u32 	%r79, %r86;

$L__BB0_11:
	mul.wide.s32 	%rd14, %r79, 8;
	add.s64 	%rd15, %rd1, %rd14;
	ld.global.nc.f64 	%fd9, [%rd15];
	setp.lt.f64 	%p17, %fd9, %fd2;
	@%p17 bra 	$L__BB0_18;
	bra.uni 	$L__BB0_12;

$L__BB0_18:
	add.s32 	%r79, %r79, 1;
	setp.lt.s32 	%p26, %r79, %r50;
	@%p26 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_19;

$L__BB0_12:
	setp.ltu.f64 	%p18, %fd9, %fd2;
	@%p18 bra 	$L__BB0_19;

	setp.lt.s32 	%p19, %r3, 1;
	add.s32 	%r84, %r84, 1;
	setp.ge.s32 	%p20, %r79, %r4;
	or.pred  	%p21, %p19, %p20;
	mov.u32 	%r86, %r79;
	@%p21 bra 	$L__BB0_19;

	mul.f64 	%fd43, %fd6, %fd45;
	mov.u32 	%r81, 0;
	mov.u32 	%r86, %r79;

$L__BB0_15:
	add.s32 	%r83, %r83, 1;
	sub.f64 	%fd45, %fd45, %fd43;
	add.s32 	%r86, %r86, 1;
	mul.wide.s32 	%rd16, %r86, 8;
	add.s64 	%rd17, %rd1, %rd16;
	ld.global.nc.f64 	%fd37, [%rd17];
	setp.ltu.f64 	%p22, %fd37, %fd3;
	@%p22 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_16;

$L__BB0_17:
	mul.f64 	%fd43, %fd4, %fd43;
	add.s32 	%r81, %r81, 1;
	setp.lt.s32 	%p23, %r81, %r3;
	setp.lt.s32 	%p24, %r86, %r4;
	and.pred  	%p25, %p23, %p24;
	@%p25 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_19;

$L__BB0_16:
	fma.rn.f64 	%fd45, %fd3, %fd43, %fd45;
	add.s32 	%r85, %r85, 1;
	max.f64 	%fd46, %fd45, %fd46;

$L__BB0_19:
	add.s32 	%r86, %r86, 1;
	setp.lt.s32 	%p27, %r86, %r50;
	@%p27 bra 	$L__BB0_4;
	bra.uni 	$L__BB0_37;

$L__BB0_20:
	mov.u32 	%r85, 0;
	mov.u32 	%r99, %r2;
	mov.u32 	%r84, %r85;
	mov.u32 	%r83, %r85;
	mov.f64 	%fd46, %fd5;
	mov.f64 	%fd45, %fd5;

$L__BB0_21:
	setp.lt.s32 	%p28, %r2, 1;
	mov.u16 	%rs8, 1;
	@%p28 bra 	$L__BB0_26;

	mov.u32 	%r91, 0;

$L__BB0_23:
	setp.le.s32 	%p30, %r99, %r91;
	mov.pred 	%p49, -1;
	@%p30 bra 	$L__BB0_25;

	not.b32 	%r71, %r91;
	add.s32 	%r72, %r99, %r71;
	mul.wide.s32 	%rd18, %r72, 8;
	add.s64 	%rd19, %rd1, %rd18;
	ld.global.nc.f64 	%fd38, [%rd19];
	setp.gt.f64 	%p49, %fd38, %fd1;

$L__BB0_25:
	not.pred 	%p31, %p49;
	selp.u16 	%rs8, 1, 0, %p31;
	add.s32 	%r91, %r91, 1;
	setp.lt.s32 	%p32, %r91, %r2;
	and.pred  	%p33, %p32, %p31;
	@%p33 bra 	$L__BB0_23;

$L__BB0_26:
	setp.ge.s32 	%p34, %r99, %r50;
	setp.eq.s16 	%p35, %rs8, 0;
	or.pred  	%p36, %p35, %p34;
	@%p36 bra 	$L__BB0_36;

	mov.u32 	%r92, %r99;

$L__BB0_28:
	mul.wide.s32 	%rd20, %r92, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.nc.f64 	%fd21, [%rd21];
	setp.lt.f64 	%p37, %fd21, %fd2;
	@%p37 bra 	$L__BB0_35;
	bra.uni 	$L__BB0_29;

$L__BB0_35:
	add.s32 	%r92, %r92, 1;
	setp.lt.s32 	%p46, %r92, %r50;
	@%p46 bra 	$L__BB0_28;
	bra.uni 	$L__BB0_36;

$L__BB0_29:
	setp.ltu.f64 	%p38, %fd21, %fd2;
	@%p38 bra 	$L__BB0_36;

	setp.lt.s32 	%p39, %r3, 1;
	add.s32 	%r84, %r84, 1;
	setp.ge.s32 	%p40, %r92, %r4;
	or.pred  	%p41, %p39, %p40;
	mov.u32 	%r99, %r92;
	@%p41 bra 	$L__BB0_36;

	mov.u32 	%r94, 0;
	mov.u32 	%r99, %r92;
	mov.f64 	%fd49, %fd32;

$L__BB0_32:
	add.s32 	%r83, %r83, 1;
	sub.f64 	%fd45, %fd45, %fd49;
	add.s32 	%r99, %r99, 1;
	mul.wide.s32 	%rd22, %r99, 8;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.nc.f64 	%fd39, [%rd23];
	setp.ltu.f64 	%p42, %fd39, %fd3;
	@%p42 bra 	$L__BB0_34;
	bra.uni 	$L__BB0_33;

$L__BB0_34:
	mul.f64 	%fd49, %fd4, %fd49;
	add.s32 	%r94, %r94, 1;
	setp.lt.s32 	%p43, %r94, %r3;
	setp.lt.s32 	%p44, %r99, %r4;
	and.pred  	%p45, %p43, %p44;
	@%p45 bra 	$L__BB0_32;
	bra.uni 	$L__BB0_36;

$L__BB0_33:
	fma.rn.f64 	%fd45, %fd3, %fd49, %fd45;
	add.s32 	%r85, %r85, 1;
	max.f64 	%fd46, %fd45, %fd46;

$L__BB0_36:
	add.s32 	%r99, %r99, 1;
	setp.lt.s32 	%p47, %r99, %r50;
	@%p47 bra 	$L__BB0_21;

$L__BB0_37:
	st.global.f64 	[%rd3+-40], %fd45;
	st.global.f64 	[%rd3+-32], %fd46;
	st.global.v2.u32 	[%rd3+-24], {%r83, %r84};
	st.global.u32 	[%rd3+-16], %r85;
	sub.f64 	%fd40, %fd45, %fd5;
	st.global.f64 	[%rd3+-8], %fd40;

$L__BB0_38:
	ret;

}

